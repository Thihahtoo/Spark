{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To run pyspark in notebook, import these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Spark\\\\spark-3.0.1-bin-hadoop2.7'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read File as csv format and take only required fields (uid, animeid, rating_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "lines = open('AnimeData/reviews.csv',encoding='utf-8')\n",
    "fcsv = csv.reader(lines)\n",
    "result = list(fcsv)\n",
    "rates = []\n",
    "for i in result[1::]:  #remove header row 0\n",
    "    rates.append((int(i[0]),int(i[2]),float(i[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(255938, 34096, 8.0),\n",
       " (259117, 34599, 10.0),\n",
       " (253664, 28891, 7.0),\n",
       " (8254, 2904, 9.0),\n",
       " (291149, 4181, 10.0),\n",
       " (10046, 2904, 10.0),\n",
       " (247454, 16664, 6.0),\n",
       " (140903, 2904, 8.0),\n",
       " (23791, 2904, 10.0),\n",
       " (25115, 4181, 4.0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates[:10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create spark session with app name 'Anime Recommendation' and create dataframe with respective column names (UID, AnimeID, Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession,functions,Row\n",
    "\n",
    "spark = SparkSession.builder.appName('Anime Recomendation').getOrCreate()\n",
    "data = spark.createDataFrame(rates,['UID', 'AnimeID', 'Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----+\n",
      "|   UID|AnimeID|Score|\n",
      "+------+-------+-----+\n",
      "|255938|  34096|  8.0|\n",
      "|259117|  34599| 10.0|\n",
      "|253664|  28891|  7.0|\n",
      "|  8254|   2904|  9.0|\n",
      "|291149|   4181| 10.0|\n",
      "| 10046|   2904| 10.0|\n",
      "|247454|  16664|  6.0|\n",
      "|140903|   2904|  8.0|\n",
      "| 23791|   2904| 10.0|\n",
      "| 25115|   4181|  4.0|\n",
      "+------+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove deplicate values of same uid rates same animeID many times by using avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+\n",
      "|   UID|AnimeID|avg(score)|\n",
      "+------+-------+----------+\n",
      "|287614|   5680|       7.0|\n",
      "| 28120|   5680|      10.0|\n",
      "| 94151|     97|      10.0|\n",
      "| 31995|   4472|       7.0|\n",
      "|297731|  38958|      10.0|\n",
      "|217503|   1571|       8.0|\n",
      "|275482|  19319|       9.0|\n",
      "|283922|  35860|       8.0|\n",
      "|240022|  28999|      10.0|\n",
      "|212633|  20057|      10.0|\n",
      "+------+-------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#remove deplicate value of same uid rates same animeID many times\n",
    "ratings = data.groupBy('UID','AnimeID').avg('score')\n",
    "ratings.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ALS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "als = ALS(rank=10, regParam=0.01, userCol='UID', itemCol='AnimeID', ratingCol='avg(score)')\n",
    "model = als.fit(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ALSModel: uid=ALS_84d3a49ac7d6, rank=10"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get anime that are more than 50 ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|AnimeID|count|\n",
      "+-------+-----+\n",
      "|     26|   79|\n",
      "|  16742|  186|\n",
      "|  30276|  414|\n",
      "|  11771|  149|\n",
      "|  35120|  314|\n",
      "|  34383|   56|\n",
      "|  13659|   53|\n",
      "|   2581|   73|\n",
      "|    270|   75|\n",
      "|   1690|   70|\n",
      "+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mostRate = ratings.groupBy('AnimeID').count().filter('count > 50')\n",
    "mostRate.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create testing dataset with UID 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|AnimeID|UID|\n",
      "+-------+---+\n",
      "|     26| 34|\n",
      "|  16742| 34|\n",
      "|  30276| 34|\n",
      "|  11771| 34|\n",
      "|  35120| 34|\n",
      "|  34383| 34|\n",
      "|  13659| 34|\n",
      "|   2581| 34|\n",
      "|    270| 34|\n",
      "|   1690| 34|\n",
      "+-------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "dataForRecom = mostRate.select('AnimeID').withColumn('UID', lit(34))\n",
    "dataForRecom.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get anime names and add it to dictionary with key as anime id and value as anime name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('AnimeData/animes.csv',encoding='utf-8')\n",
    "names = list(csv.reader(file))\n",
    "anime_names = {}\n",
    "for name in names[1::]:\n",
    "    anime_names[int(name[0])] = name[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the data of UID 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+\n",
      "|AnimeID|UID|prediction|\n",
      "+-------+---+----------+\n",
      "|  28891| 34|  4.599096|\n",
      "|  10495| 34| 3.3498428|\n",
      "|   8426| 34| 3.2341256|\n",
      "|  16774| 34| 3.1218092|\n",
      "|   6802| 34| 2.9618676|\n",
      "|  34984| 34|  2.934247|\n",
      "|  38329| 34| 2.9225383|\n",
      "|     45| 34|  2.904904|\n",
      "|     57| 34|  2.898753|\n",
      "|  21877| 34| 2.5509806|\n",
      "+-------+---+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = model.transform(dataForRecom)\n",
    "recomemded = result.sort('prediction',ascending=False)\n",
    "recomemded.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get top 10 prediction with anime names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show = recomemded.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recomended Anime for userid 34:\n",
      "\n",
      "Haikyuu!! Second Season 4.6\n",
      "Yuru Yuri 3.35\n",
      "Hourou Musuko 3.23\n",
      "Inferno Cop 3.12\n",
      "So Ra No Wo To 2.96\n",
      "Koi wa Ameagari no You ni 2.93\n",
      "Seishun Buta Yarou wa Yumemiru Shoujo no Yume wo Minai 2.92\n",
      "Rurouni Kenshin: Meiji Kenkaku Romantan 2.9\n",
      "Beck 2.9\n",
      "High Score Girl 2.55\n"
     ]
    }
   ],
   "source": [
    "print('Recomended Anime for userid 34:\\n')\n",
    "\n",
    "for anime in show:\n",
    "    print(anime_names[anime[0]],round(anime[2],2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
